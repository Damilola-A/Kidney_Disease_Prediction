# -*- coding: utf-8 -*-
"""Kidney_Disease_pridiction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J8mSYP5_y-1MWAz-vmTCZmb4Z9nrDblK

LAB-PHASE PROJECT

Objectives of these project:

The objective of this lab phase is to allow students to practice and validate their acquired skills on any regression or classification topic they want to work on. Students are free to propose their project ideas that fall under supervised machine learning tasks. We highly appreciate innovate ideas that showcase your skills and propose inventive solutions to recurrent problems.
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

"""DATASET DESCRIPTION


The dataset is taken over 2-month period in India. It has 400 rows with 25 features like red blood cells, pedal edema, sugar,etc. The aim is to classify whether a patient has chronic kidney disease or not. The classification is based on a attribute named 'classification' which is either 'ckd'(chronic kidney disease) or 'notckd. I've performed cleaning of the dataset which includes mapping the text to numbers and some other changes. After the cleaning I've done some EDA(Exploratory Data Analysis) and then I've divided the dataset int training and testing and applied the models on them. It is observed that the classification results are not much satisfying initially. So, instead of dropping the rows with Nan values I've used the lambda function to replace them with mode for each column. After that I've divided the dataset again into training and testing sets and applied models on them. This time the results are better and we see that the random forest and decision trees are the best performers with an accuracy of 1.0 and 0 misclassifications. The performance of the classification is measured by printing confusion matrix, classification report and accuracy.
"""

kd = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/kidney_disease.csv')

kd.rename(columns={'bp': 'blood_pressure', 'sg': 'specific gravity','al':'albumin','su':'sugar',
                      'rbc':'red blood cells','pc':'pus cell','pcc':'pus cell clumps','ba':'bacteria',
                      'bgr':'blood glucose random','bu':'blood urea','sc':'serum creatinine','sod':'sodium',
                      'pot':'potassium','hemo':'hemoglobin','pcv':'packed cell volume','wc':'white blood cell count',
                      'rc':'red blood cell count','htn':'hypertension','dm':'diabetes mellitus',
                      'cad':'coronary artery disease','appet':'appetite','pe':'pedal edema',
                      'ane':'anemia','classification':'class'}, inplace=True)

pd.set_option('display.max_columns', None)
kd.head()

kd.isnull().sum()

kd.drop(['id'], inplace = True, axis = 1)

cat = kd.select_dtypes(include = ['object', 'category'])
num = kd.select_dtypes(include = 'number')

display(cat.head())
display(num.head())

df = kd.copy()

# select categorical columns that are numbers
bad_cat_cols = ['packed cell volume',	'white blood cell count',	'red blood cell count']

for i in bad_cat_cols:
  df[i] = pd.to_numeric(df[i], errors = 'coerce')

new_cats = df.select_dtypes(include = ['category', 'object'])
new_num = df.select_dtypes(include = 'number')

print('\t \t \tCATEGORICAL COLUMNS')
display(new_cats.head())

print(f"\t \t \t \n \n NUMERICAL COLUMNS")
display(new_num.head())

for i in new_cats.columns:
  df[i].fillna(df[i].mode()[0], inplace = True)

for i in new_num.columns:
  df[i].fillna(df[i].median(), inplace = True)

df.isnull().sum()

from sklearn.preprocessing import LabelEncoder, StandardScaler
encoder = LabelEncoder()
scaler = StandardScaler()

for i in new_cats:
  df[i] = encoder.fit_transform(df[i])

for i in new_num:
  df[[i]] = scaler.fit_transform(df[[i]])

df.head()

# Prepare the data
X = df.drop(['class'], axis=1)
y = df['class']


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Apply logistic regression
lr = LogisticRegression(random_state=0)
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

# Print the confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize = (3,2))
sns.set(style = 'darkgrid')
sns.heatmap(cm/np.sum(cm), annot=True, cmap='crest', fmt='.1%', linewidth=1)

# Find the optimal number of neighbors
accuracy = []
for i in range(1, 11):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy.append((y_pred == y_test).mean())

plt.plot(range(1, 11), accuracy)
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.show()

# Apply decision tree
dt = DecisionTreeClassifier(random_state=0)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)

# Plot the decision tree
from sklearn.tree import plot_tree
plt.figure(figsize=(20, 10))
plot_tree(dt, filled=True)
plt.show()

# Calculate the accuracy
accuracy = (y_pred == y_test).mean()
print('Accuracy:', accuracy)

# Apply random forest
rf = RandomForestClassifier(random_state=0)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

# Calculate the accuracy
accuracy = (y_pred == y_test).mean()
print('Accuracy:', accuracy)